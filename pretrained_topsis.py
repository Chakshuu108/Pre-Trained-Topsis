# -*- coding: utf-8 -*-
"""Untitled16.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c7nxMoRMq0PMpWWfAC7HdecTO5oExBVt
"""

import time
import os
import torch
import numpy as np
import pandas as pd
from transformers import AutoModelForCausalLM, AutoTokenizer
from tqdm import tqdm

device = "cuda" if torch.cuda.is_available() else "cpu"

models = [
    "gpt2",
    "distilgpt2",
    "facebook/opt-350m",
    "EleutherAI/gpt-neo-125M",
    "facebook/opt-125m"
]

test_text = "Artificial intelligence is transforming the world because it allows machines to learn from data and improve over time."

gen_tokens = 50

def count_parameters(model):
    return sum(p.numel() for p in model.parameters())

def get_model_size_on_disk(model_name):
    from transformers.utils import cached_file
    try:
        weights_file = cached_file(model_name, "pytorch_model.bin")
        return os.path.getsize(weights_file) / (1024 * 1024)
    except:
        return np.nan

def compute_perplexity(model, tokenizer, text):
    encodings = tokenizer(text, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = model(**encodings, labels=encodings["input_ids"])
        loss = outputs.loss
    ppl = torch.exp(loss).item()
    return ppl

results = []

for model_name in tqdm(models, desc="Evaluating models"):
    print(f"\nLoading {model_name} ...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)
    model.eval()

    num_params = count_parameters(model)
    size_mb = get_model_size_on_disk(model_name)

    inputs = tokenizer(test_text, return_tensors="pt").to(device)

    with torch.no_grad():
        _ = model.generate(**inputs, max_new_tokens=10)

    torch.cuda.synchronize() if device == "cuda" else None
    start = time.time()
    with torch.no_grad():
        output = model.generate(**inputs, max_new_tokens=gen_tokens)
    torch.cuda.synchronize() if device == "cuda" else None
    end = time.time()

    latency = end - start
    tokens_generated = output.shape[1] - inputs["input_ids"].shape[1]
    tokens_per_sec = tokens_generated / latency

    ppl = compute_perplexity(model, tokenizer, test_text)

    results.append({
        "Model": model_name,
        "Params": num_params,
        "Size_MB": size_mb,
        "Latency_s": latency,
        "Tokens_per_s": tokens_per_sec,
        "Perplexity": ppl
    })

df = pd.DataFrame(results)

print("\nRaw Metrics:\n", df)

criteria = ["Params", "Size_MB", "Latency_s", "Tokens_per_s", "Perplexity"]

D = df[criteria].astype(float).values

weights = np.array([0.2, 0.2, 0.2, 0.2, 0.2])

impacts = np.array([-1, -1, -1, +1, -1])

norm = D / np.sqrt((D**2).sum(axis=0))

V = norm * weights

ideal_best = np.where(impacts == 1, V.max(axis=0), V.min(axis=0))
ideal_worst = np.where(impacts == 1, V.min(axis=0), V.max(axis=0))

S_best = np.sqrt(((V - ideal_best)**2).sum(axis=1))
S_worst = np.sqrt(((V - ideal_worst)**2).sum(axis=1))

topsis_score = S_worst / (S_best + S_worst)

df["TOPSIS_Score"] = topsis_score
df["Rank"] = df["TOPSIS_Score"].rank(ascending=False, method="dense")

df_sorted = df.sort_values(by="TOPSIS_Score", ascending=False)

print("\nTOPSIS Ranking:\n", df_sorted)

df_sorted.to_csv("topsis_text_generation_results.csv", index=False)
print("\nSaved to topsis_text_generation_results.csv")

import matplotlib.pyplot as plt

# -------- Graph 1: Tokens per second per model --------
plt.figure()
plt.bar(df_sorted["Model"], df_sorted["Tokens_per_s"])
plt.xlabel("Model")
plt.ylabel("Tokens per second")
plt.title("Generation Speed Comparison (Tokens/s)")
plt.xticks(rotation=45, ha="right")
plt.tight_layout()
plt.show()

# -------- Graph 2: TOPSIS Score per model --------
plt.figure()
plt.bar(df_sorted["Model"], df_sorted["TOPSIS_Score"])
plt.xlabel("Model")
plt.ylabel("TOPSIS Score")
plt.title("Overall Model Ranking (TOPSIS Score)")
plt.xticks(rotation=45, ha="right")
plt.tight_layout()
plt.show()

